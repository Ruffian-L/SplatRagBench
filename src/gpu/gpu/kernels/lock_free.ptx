//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34097967
// Cuda compilation tools, release 12.4, V12.4.131
// Based on NVVM 7.0.1
//

.version 8.4
.target sm_86
.address_size 64

	// .globl	lock_free_reduction
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.global .align 1 .b8 $str[15] = {71, 80, 85, 32, 72, 101, 97, 112, 32, 79, 79, 77, 33, 10};

.visible .entry lock_free_reduction(
	.param .u64 lock_free_reduction_param_0,
	.param .u64 lock_free_reduction_param_1,
	.param .u64 lock_free_reduction_param_2,
	.param .u64 lock_free_reduction_param_3,
	.param .u64 lock_free_reduction_param_4,
	.param .u64 lock_free_reduction_param_5,
	.param .u32 lock_free_reduction_param_6,
	.param .u32 lock_free_reduction_param_7
)
{
	.reg .pred 	%p<40>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<153>;
	.reg .b64 	%rd<80>;


	ld.param.u64 	%rd22, [lock_free_reduction_param_0];
	ld.param.u64 	%rd25, [lock_free_reduction_param_1];
	ld.param.u64 	%rd26, [lock_free_reduction_param_2];
	ld.param.u64 	%rd23, [lock_free_reduction_param_3];
	ld.param.u64 	%rd27, [lock_free_reduction_param_4];
	ld.param.u64 	%rd24, [lock_free_reduction_param_5];
	ld.param.u32 	%r68, [lock_free_reduction_param_6];
	ld.param.u32 	%r67, [lock_free_reduction_param_7];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	cvta.to.global.u64 	%rd3, %rd25;
	mov.u32 	%r69, %ntid.x;
	mov.u32 	%r70, %ctaid.x;
	mov.u32 	%r71, %tid.x;
	mad.lo.s32 	%r72, %r70, %r69, %r71;
	shr.u32 	%r1, %r72, 5;
	and.b32  	%r2, %r71, 31;
	setp.ge.s32 	%p1, %r1, %r68;
	@%p1 bra 	$L__BB0_39;

	cvta.to.global.u64 	%rd28, %rd23;
	cvt.u64.u32 	%rd29, %r1;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u8 	%rs1, [%rd30];
	setp.ne.s16 	%p2, %rs1, 0;
	@%p2 bra 	$L__BB0_39;
	bra.uni 	$L__BB0_2;

$L__BB0_39:
	ret;

$L__BB0_2:
	mul.wide.u32 	%rd31, %r1, 4;
	add.s64 	%rd32, %rd3, %rd31;
	ld.global.nc.u32 	%r74, [%rd32];
	ld.global.nc.u32 	%r75, [%rd32+4];
	sub.s32 	%r152, %r75, %r74;
	mul.wide.s32 	%rd33, %r74, 4;
	add.s64 	%rd77, %rd2, %rd33;
	cvta.to.global.u64 	%rd5, %rd24;
	cvta.to.global.u64 	%rd6, %rd22;
	mov.u32 	%r127, 1;

$L__BB0_3:
	mov.u32 	%r4, %r127;
	setp.lt.s32 	%p3, %r152, 1;
	setp.ne.s32 	%p4, %r2, 0;
	mov.u32 	%r76, -1;
	or.pred  	%p5, %p4, %p3;
	mov.u32 	%r129, %r76;
	@%p5 bra 	$L__BB0_5;

	ld.global.nc.u32 	%r129, [%rd77];

$L__BB0_5:
	mov.u32 	%r77, 31;
	mov.u32 	%r78, 0;
	shfl.sync.idx.b32 	%r8|%p6, %r129, %r78, %r77, %r76;
	setp.eq.s32 	%p7, %r8, -1;
	@%p7 bra 	$L__BB0_39;

	mov.u32 	%r80, -1;
	mov.u32 	%r130, %r80;
	@%p4 bra 	$L__BB0_8;

	mul.wide.s32 	%rd34, %r8, 4;
	add.s64 	%rd35, %rd6, %rd34;
	mov.u32 	%r81, -1;
	atom.global.cas.b32 	%r130, [%rd35], %r81, %r1;

$L__BB0_8:
	mov.u32 	%r82, 31;
	mov.u32 	%r83, 0;
	shfl.sync.idx.b32 	%r11|%p9, %r130, %r83, %r82, %r80;
	setp.eq.s32 	%p10, %r11, -1;
	setp.eq.s32 	%p11, %r11, %r1;
	or.pred  	%p12, %p10, %p11;
	@%p12 bra 	$L__BB0_39;

	mul.wide.s32 	%rd36, %r11, 4;
	add.s64 	%rd37, %rd3, %rd36;
	ld.global.nc.u32 	%r12, [%rd37+4];
	ld.global.nc.u32 	%r86, [%rd37];
	sub.s32 	%r13, %r12, %r86;
	cvt.s64.s32 	%rd8, %r86;
	add.s32 	%r14, %r13, %r152;
	mov.u32 	%r85, -1;
	mov.u32 	%r131, %r85;
	@%p4 bra 	$L__BB0_11;

	atom.global.add.u32 	%r131, [%rd5], %r14;

$L__BB0_11:
	mov.u32 	%r87, 31;
	mov.u32 	%r88, 0;
	shfl.sync.idx.b32 	%r17|%p14, %r131, %r88, %r87, %r85;
	add.s32 	%r90, %r17, %r14;
	setp.lt.s32 	%p15, %r90, %r67;
	@%p15 bra 	$L__BB0_14;
	bra.uni 	$L__BB0_12;

$L__BB0_14:
	cvt.s64.s32 	%rd9, %r17;
	@%p4 bra 	$L__BB0_38;

	mov.u32 	%r135, 0;
	setp.lt.s32 	%p19, %r13, 1;
	or.pred  	%p20, %p3, %p19;
	mov.u32 	%r136, %r135;
	mov.u32 	%r137, %r135;
	@%p20 bra 	$L__BB0_23;

	mov.u32 	%r137, 0;
	mov.u32 	%r136, %r137;
	mov.u32 	%r135, %r137;

$L__BB0_17:
	mul.wide.s32 	%rd40, %r135, 4;
	add.s64 	%rd41, %rd77, %rd40;
	cvt.s64.s32 	%rd42, %r136;
	add.s64 	%rd43, %rd42, %rd8;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd45, %rd2, %rd44;
	ld.global.nc.u32 	%r21, [%rd45];
	ld.global.nc.u32 	%r22, [%rd41];
	setp.gt.s32 	%p21, %r22, %r21;
	cvt.s64.s32 	%rd46, %r137;
	add.s64 	%rd47, %rd46, %rd9;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd10, %rd1, %rd48;
	@%p21 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_18;

$L__BB0_21:
	add.s32 	%r137, %r137, 1;
	st.global.u32 	[%rd10], %r22;
	add.s32 	%r135, %r135, 1;
	bra.uni 	$L__BB0_22;

$L__BB0_18:
	setp.gt.s32 	%p22, %r21, %r22;
	@%p22 bra 	$L__BB0_20;
	bra.uni 	$L__BB0_19;

$L__BB0_20:
	add.s32 	%r137, %r137, 1;
	st.global.u32 	[%rd10], %r21;
	add.s32 	%r136, %r136, 1;
	bra.uni 	$L__BB0_22;

$L__BB0_19:
	add.s32 	%r135, %r135, 1;
	add.s32 	%r136, %r136, 1;

$L__BB0_22:
	setp.lt.s32 	%p23, %r136, %r13;
	setp.lt.s32 	%p24, %r135, %r152;
	and.pred  	%p25, %p24, %p23;
	@%p25 bra 	$L__BB0_17;

$L__BB0_23:
	setp.le.s32 	%p26, %r152, %r135;
	mov.u32 	%r146, %r137;
	@%p26 bra 	$L__BB0_31;

	sub.s32 	%r99, %r152, %r135;
	and.b32  	%r35, %r99, 3;
	setp.eq.s32 	%p27, %r35, 0;
	mov.u32 	%r146, %r137;
	mov.u32 	%r142, %r135;
	@%p27 bra 	$L__BB0_28;

	add.s32 	%r142, %r135, 1;
	mul.wide.s32 	%rd49, %r135, 4;
	add.s64 	%rd11, %rd77, %rd49;
	ld.global.nc.u32 	%r100, [%rd11];
	add.s32 	%r146, %r137, 1;
	cvt.s64.s32 	%rd50, %r137;
	add.s64 	%rd51, %rd50, %rd9;
	shl.b64 	%rd52, %rd51, 2;
	add.s64 	%rd12, %rd1, %rd52;
	st.global.u32 	[%rd12], %r100;
	setp.eq.s32 	%p28, %r35, 1;
	@%p28 bra 	$L__BB0_28;

	add.s32 	%r142, %r135, 2;
	ld.global.nc.u32 	%r101, [%rd11+4];
	add.s32 	%r146, %r137, 2;
	st.global.u32 	[%rd12+4], %r101;
	setp.eq.s32 	%p29, %r35, 2;
	@%p29 bra 	$L__BB0_28;

	add.s32 	%r142, %r135, 3;
	ld.global.nc.u32 	%r102, [%rd11+8];
	add.s32 	%r146, %r137, 3;
	st.global.u32 	[%rd12+8], %r102;

$L__BB0_28:
	not.b32 	%r103, %r135;
	add.s32 	%r104, %r152, %r103;
	setp.lt.u32 	%p30, %r104, 3;
	@%p30 bra 	$L__BB0_31;

	cvt.s64.s32 	%rd53, %r146;
	add.s64 	%rd54, %rd9, %rd53;
	shl.b64 	%rd55, %rd54, 2;
	add.s64 	%rd79, %rd1, %rd55;
	mul.wide.s32 	%rd56, %r142, 4;
	add.s64 	%rd57, %rd77, %rd56;
	add.s64 	%rd78, %rd57, 8;

$L__BB0_30:
	ld.global.nc.u32 	%r105, [%rd78+-8];
	st.global.u32 	[%rd79], %r105;
	ld.global.nc.u32 	%r106, [%rd78+-4];
	st.global.u32 	[%rd79+4], %r106;
	ld.global.nc.u32 	%r107, [%rd78];
	st.global.u32 	[%rd79+8], %r107;
	ld.global.nc.u32 	%r108, [%rd78+4];
	add.s32 	%r146, %r146, 4;
	st.global.u32 	[%rd79+12], %r108;
	add.s64 	%rd79, %rd79, 16;
	add.s64 	%rd78, %rd78, 16;
	add.s32 	%r142, %r142, 4;
	setp.lt.s32 	%p31, %r142, %r152;
	@%p31 bra 	$L__BB0_30;

$L__BB0_31:
	setp.ge.s32 	%p32, %r136, %r13;
	mov.u32 	%r152, %r146;
	@%p32 bra 	$L__BB0_38;

	cvt.u32.u64 	%r110, %rd8;
	sub.s32 	%r111, %r12, %r136;
	sub.s32 	%r112, %r111, %r110;
	and.b32  	%r50, %r112, 3;
	setp.eq.s32 	%p33, %r50, 0;
	mov.u32 	%r152, %r146;
	mov.u32 	%r148, %r136;
	@%p33 bra 	$L__BB0_36;

	add.s32 	%r148, %r136, 1;
	cvt.s64.s32 	%rd58, %r136;
	add.s64 	%rd59, %rd58, %rd8;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	ld.global.nc.u32 	%r113, [%rd61];
	add.s32 	%r152, %r146, 1;
	cvt.s64.s32 	%rd62, %r146;
	add.s64 	%rd63, %rd62, %rd9;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd19, %rd1, %rd64;
	st.global.u32 	[%rd19], %r113;
	setp.eq.s32 	%p34, %r50, 1;
	@%p34 bra 	$L__BB0_36;

	add.s32 	%r53, %r136, 2;
	cvt.s64.s32 	%rd65, %r148;
	add.s64 	%rd66, %rd65, %rd8;
	shl.b64 	%rd67, %rd66, 2;
	add.s64 	%rd20, %rd2, %rd67;
	ld.global.nc.u32 	%r114, [%rd20];
	add.s32 	%r152, %r146, 2;
	st.global.u32 	[%rd19+4], %r114;
	setp.eq.s32 	%p35, %r50, 2;
	mov.u32 	%r148, %r53;
	@%p35 bra 	$L__BB0_36;

	add.s32 	%r148, %r136, 3;
	ld.global.nc.u32 	%r115, [%rd20+4];
	add.s32 	%r152, %r146, 3;
	st.global.u32 	[%rd19+8], %r115;

$L__BB0_36:
	not.b32 	%r117, %r136;
	add.s32 	%r118, %r12, %r117;
	sub.s32 	%r119, %r118, %r110;
	setp.lt.u32 	%p36, %r119, 3;
	@%p36 bra 	$L__BB0_38;

$L__BB0_37:
	cvt.s64.s32 	%rd68, %r148;
	add.s64 	%rd69, %rd68, %rd8;
	shl.b64 	%rd70, %rd69, 2;
	add.s64 	%rd71, %rd2, %rd70;
	ld.global.nc.u32 	%r120, [%rd71];
	cvt.s64.s32 	%rd72, %r152;
	add.s64 	%rd73, %rd72, %rd9;
	shl.b64 	%rd74, %rd73, 2;
	add.s64 	%rd75, %rd1, %rd74;
	st.global.u32 	[%rd75], %r120;
	ld.global.nc.u32 	%r121, [%rd71+4];
	st.global.u32 	[%rd75+4], %r121;
	ld.global.nc.u32 	%r122, [%rd71+8];
	st.global.u32 	[%rd75+8], %r122;
	ld.global.nc.u32 	%r123, [%rd71+12];
	add.s32 	%r152, %r152, 4;
	st.global.u32 	[%rd75+12], %r123;
	add.s32 	%r148, %r148, 4;
	setp.lt.s32 	%p37, %r148, %r13;
	@%p37 bra 	$L__BB0_37;

$L__BB0_38:
	mov.u32 	%r124, 31;
	mov.u32 	%r125, 0;
	mov.u32 	%r126, -1;
	shfl.sync.idx.b32 	%r152|%p38, %r152, %r125, %r124, %r126;
	shl.b64 	%rd76, %rd9, 2;
	add.s64 	%rd77, %rd1, %rd76;
	add.s32 	%r127, %r4, 1;
	setp.lt.u32 	%p39, %r4, 10000;
	@%p39 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_39;

$L__BB0_12:
	@%p4 bra 	$L__BB0_39;

	mov.u64 	%rd38, $str;
	cvta.global.u64 	%rd39, %rd38;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd39;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r91, [retval0+0];
	} // callseq 0
	bra.uni 	$L__BB0_39;

}

